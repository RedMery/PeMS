{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras import losses\n",
    "import shutil\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_D_station = np.load('part_D_station.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = os.listdir('dataImage/')\n",
    "fileNames.sort()\n",
    "filePaths = [os.path.join(os.path.abspath('dataImage/'), fileName) for fileName in fileNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 发送消息\n",
    "from twilio.rest import Client\n",
    "def sendMessage(sentInfo):\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = \"ACc807eaedbd160252f16250798533706f\"\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = \"3199024dc7475dcd653f98a40cd16648\"\n",
    "    client = Client(account_sid, auth_token)\n",
    "    message = client.messages.create(\n",
    "    # 这里中国的号码前面需要加86\n",
    "        to=\"8613246857840\", \n",
    "        from_=\"(425) 341-1571 \",\n",
    "        body=\"{0}\".format(sentInfo))\n",
    "#     print(message.sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生数据，返回数据为(timeStep, 56, 56, 1),内容索引为\n",
    "\n",
    "def data_part(origin_array, target_array):\n",
    "    l = []\n",
    "    origin_array = np.reshape(origin_array, (-1, 3))\n",
    "    for i in range(len(origin_array)):\n",
    "        if(origin_array[i][0] in target_array):\n",
    "            l.append(origin_array[i])\n",
    "    array = np.array(l)\n",
    "    array = np.reshape(array, (28, 28, 3))\n",
    "    return array\n",
    "\n",
    "def dataGenTimeStep(filePaths, start, timeStep):\n",
    "    l = []\n",
    "    for i in range(timeStep):\n",
    "        data = np.load(filePaths[start+i])\n",
    "#         print (filePaths[start+i], '读取完毕')  # 输出一个batch中的文件\n",
    "        data = data[:,:,1]\n",
    "        data = data[:,:,np.newaxis]\n",
    "        l.append(data)\n",
    "    dataArray = np.array(l)\n",
    "    return dataArray\n",
    "\n",
    "# 产生一个batch_size的数据\n",
    "def dataGenBatchSize(filePaths, timeStep, start, predictFutureTime, batch_size, station_array):\n",
    "    l = []\n",
    "    m = []\n",
    "    for i in range(batch_size):    \n",
    "        data = dataGenTimeStep(filePaths, start + i , timeStep)\n",
    "        l.append(data)\n",
    "        targetIndex = start + i + timeStep - 1 + predictFutureTime/5\n",
    "        targetIndex = int(targetIndex)\n",
    "        target = np.load(filePaths[targetIndex])\n",
    "#         print ('索引文件为', filePaths[targetIndex])    #测试目标文件是否正确\n",
    "\n",
    "        target = data_part(target, station_array)\n",
    "        \n",
    "        target = target[:,:,1]\n",
    "        target = target[:,:,np.newaxis]\n",
    "        m.append(target)\n",
    "    data = np.array(l)\n",
    "    target = np.array(m)\n",
    "    return (data, target)\n",
    "\n",
    "# timeStep---预测的时间步\n",
    "# predictFutureTime--预测未来的时间\n",
    "# batch_size -- 每次输入的batch，更新一次\n",
    "def dataGenerate(filePaths, timeStep, predictFutureTime, batch_size, station_array):\n",
    "    while True:\n",
    "        end_index = len(filePaths) - timeStep - 3\n",
    "        L = np.arange(0, end_index, batch_size)\n",
    "        for start in L:\n",
    "            if start < L[-1]:\n",
    "                data, target = dataGenBatchSize(filePaths, timeStep, start, predictFutureTime, batch_size, station_array)\n",
    "            if start == L[-1]:\n",
    "                batch_size_ = end_index - L[-1] + 1\n",
    "                data, target = dataGenBatchSize(filePaths, timeStep, start, predictFutureTime, batch_size, station_array)\n",
    "            yield (data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数 MSPE\n",
    "\n",
    "def mean_squared_percentage_error(y_true, y_pred):\n",
    "    if not K.is_tensor(y_pred):\n",
    "        y_pred = K.constant(y_pred, dtype = 'float64')\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "    return K.mean(K.square((y_pred - y_true)/K.clip(y_true, K.epsilon(), None)))\n",
    "\n",
    "\n",
    "# 定义评价函数metrics, MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    if not K.is_tensor(y_pred):\n",
    "        y_pred = K.constant(y_pred, dtype = 'float64')\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "    return K.mean(K.abs((y_true - y_pred) / K.clip(K.abs(y_true), K.epsilon(), None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [mean_absolute_percentage_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Sequential()\n",
    "\n",
    "seq.add(ConvLSTM2D(filters = 32, kernel_size = (3, 3),\n",
    "                   input_shape = (None, 56, 56, 1),\n",
    "                   padding = 'same', return_sequences = True))\n",
    "\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "\n",
    "seq.add(ConvLSTM2D(filters = 32, kernel_size = (3, 3),\n",
    "                   padding = 'same', return_sequences = True))\n",
    "\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters = 1, kernel_size = (3, 3), strides = (2, 2),\n",
    "                   padding = 'same', return_sequences = False))\n",
    "\n",
    "## 损失函数定义为 mean_squared_percentage_error\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "seq.compile(loss = mean_squared_percentage_error, optimizer = adam, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "savename = \"{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "os.mkdir('model')\n",
    "# 中途训练效果提升, 则将文件保存, 每提升一次, 保存一次\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(os.path.join('model', savename), monitor='val_loss', verbose=1, save_best_only=False, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_4 (ConvLSTM2D)  (None, None, 56, 56, 32)  38144     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 56, 56, 32)  128       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_5 (ConvLSTM2D)  (None, None, 56, 56, 32)  73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 56, 56, 32)  128       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  (None, 28, 28, 1)         1192      \n",
      "=================================================================\n",
      "Total params: 113,448\n",
      "Trainable params: 113,320\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq.summary()\n",
    "# plot_model(seq, show_layer_names = True, show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq.metrics_names\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.mape = []\n",
    "\n",
    "#         print ()\n",
    "#         print ('*'*10, 'batch begin', '*'*10)\n",
    "#         print (\"logs:\", logs)\n",
    "#         print ('*'*10, 'batch begin', '*'*10)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.mape.append(logs.get('mean_absolute_percentage_error'))\n",
    "\n",
    "#         print ()\n",
    "#         print ('*'*10, 'batch end', '*'*10)\n",
    "#         print (\"logs:\", logs)\n",
    "#         print ('*'*10, 'batch end', '*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(filePaths)\n",
    "EPOCHS = 10\n",
    "timeStep = 36\n",
    "predictFutureTime = 15\n",
    "batch_size = 16\n",
    "\n",
    "history_train = LossHistory()\n",
    "station_array = np.load('part_D_station.npy')\n",
    "# 训练数据集\n",
    "# dataGenerate(filePaths, timeStep, predictFutureTime, batch_size)\n",
    "trainStart = 0\n",
    "trainEnd = math.floor(total/10*8)\n",
    "train_steps = math.ceil((trainEnd - trainStart)/batch_size)\n",
    "train_file_paths = filePaths[trainStart:trainEnd]\n",
    "train_generator = dataGenerate(train_file_paths, timeStep, predictFutureTime, batch_size, station_array = station_array)\n",
    "\n",
    "# 验证数据\n",
    "valStart = math.floor(total/10*8)\n",
    "valEnd = math.floor(total/10*9)\n",
    "val_steps = math.ceil((valEnd - valStart)/batch_size)\n",
    "val_file_paths = filePaths[valStart:valEnd]\n",
    "val_generator = dataGenerate(val_file_paths, timeStep, predictFutureTime, batch_size, station_array = station_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'mean_absolute_percentage_error']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/redmery/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/redmery/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/6\n",
      "1296/1296 [==============================] - 75s 58ms/step - loss: 586011103.7004 - mean_absolute_percentage_error: 279.0171 - val_loss: 139.9652 - val_mean_absolute_percentage_error: 4.4162\n",
      "\n",
      "Epoch 00001: saving model to weights-improvement-01-139.97.hdf5\n",
      "Epoch 2/6\n",
      "1296/1296 [==============================] - 73s 57ms/step - loss: 351478.9355 - mean_absolute_percentage_error: 3.5284 - val_loss: 5.8417 - val_mean_absolute_percentage_error: 2.2486\n",
      "\n",
      "Epoch 00002: saving model to weights-improvement-02-5.84.hdf5\n",
      "Epoch 3/6\n",
      " 336/1296 [======>.......................] - ETA: 52s - loss: 107347.4662 - mean_absolute_percentage_error: 2.3777"
     ]
    }
   ],
   "source": [
    "historyTrainEpoch = seq.fit_generator(generator = train_generator, steps_per_epoch = train_steps, \\\n",
    "                      epochs = EPOCHS, callbacks = [history_train, checkpoint], validation_data = val_generator, \\\n",
    "                      validation_steps = val_steps, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'historyTrainEpoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0dbd5fc91167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## save history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistoryTrainEpoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## transfom the data to json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMyEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'historyTrainEpoch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "## save history\n",
    "print (historyTrainEpoch.history)\n",
    "\n",
    "## transfom the data to json\n",
    "class MyEncoder(json.JSONEncoder):\n",
    " \n",
    "    def default(self, obj):\n",
    "        \"\"\"\n",
    "        只要检查到了是bytes类型的数据就把它转为str类型\n",
    "        :param obj:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(obj, np.float32):\n",
    "            return str(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# 保存训练过程中的loss，以及验证过程中的loss、mspe\n",
    "with open('process_data_each_time.json', 'w') as f:\n",
    "    json.dump(historyTrainEpoch.history, f, cls=MyEncoder)\n",
    "np.save('history_train_loss.npy', np.array(history_train.losses))\n",
    "np.save('history_train_mape.npy', np.array(history_train.mape))\n",
    "\n",
    "# 保存模型(使用callback对每轮的迭代进行保存，因此不用该方法保存)\n",
    "# if os.path.exists('model'):\n",
    "#     shutil.rmtree('model')\n",
    "# os.makedirs('model')\n",
    "# model_path = os.path.join('model','model.h5')\n",
    "# seq.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  加载模型(模型中自定义的函数，需要使用custom_objects进行指明)\n",
    "# model_path = os.path.join('model/model' ,'model.h5')\n",
    "# seq = load_model(model_path, custom_objects={'my_loss': my_loss,'mean_squared_percentage_error':mean_squared_percentage_error})\n",
    "# history_evaluate = LossHistory()\n",
    "\n",
    "# # 测试数据\n",
    "# testStart = math.floor(total/10*9)\n",
    "# testEnd = math.floor(total)\n",
    "# test_steps = math.ceil((testEnd - testStart)/batch_size)\n",
    "# test_file_paths = filePaths[testStart:testEnd]\n",
    "# test_data = dataGenerate(test_file_paths, timeStep, predictFutureTime, batch_size)\n",
    "\n",
    "# seq.summary()\n",
    "# result_evaluate = seq.evaluate_generator(test_data, steps = test_steps, \\\n",
    "#                                               callbacks = [history_evaluate], verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mConvLSTM2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrecurrent_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hard_sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrecurrent_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orthogonal'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0munit_forget_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrecurrent_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrecurrent_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgo_backwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Convolutional LSTM.\n",
       "\n",
       "It is similar to an LSTM layer, but the input transformations\n",
       "and recurrent transformations are both convolutional.\n",
       "\n",
       "# Arguments\n",
       "    filters: Integer, the dimensionality of the output space\n",
       "        (i.e. the number output of filters in the convolution).\n",
       "    kernel_size: An integer or tuple/list of n integers, specifying the\n",
       "        dimensions of the convolution window.\n",
       "    strides: An integer or tuple/list of n integers,\n",
       "        specifying the strides of the convolution.\n",
       "        Specifying any stride value != 1 is incompatible with specifying\n",
       "        any `dilation_rate` value != 1.\n",
       "    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n",
       "    data_format: A string,\n",
       "        one of `\"channels_last\"` (default) or `\"channels_first\"`.\n",
       "        The ordering of the dimensions in the inputs.\n",
       "        `\"channels_last\"` corresponds to inputs with shape\n",
       "        `(batch, time, ..., channels)`\n",
       "        while `\"channels_first\"` corresponds to\n",
       "        inputs with shape `(batch, time, channels, ...)`.\n",
       "        It defaults to the `image_data_format` value found in your\n",
       "        Keras config file at `~/.keras/keras.json`.\n",
       "        If you never set it, then it will be `\"channels_last\"`.\n",
       "    dilation_rate: An integer or tuple/list of n integers, specifying\n",
       "        the dilation rate to use for dilated convolution.\n",
       "        Currently, specifying any `dilation_rate` value != 1 is\n",
       "        incompatible with specifying any `strides` value != 1.\n",
       "    activation: Activation function to use\n",
       "        (see [activations](../activations.md)).\n",
       "    recurrent_activation: Activation function to use\n",
       "        for the recurrent step\n",
       "        (see [activations](../activations.md)).\n",
       "    use_bias: Boolean, whether the layer uses a bias vector.\n",
       "    kernel_initializer: Initializer for the `kernel` weights matrix,\n",
       "        used for the linear transformation of the inputs.\n",
       "        (see [initializers](../initializers.md)).\n",
       "    recurrent_initializer: Initializer for the `recurrent_kernel`\n",
       "        weights matrix,\n",
       "        used for the linear transformation of the recurrent state.\n",
       "        (see [initializers](../initializers.md)).\n",
       "    bias_initializer: Initializer for the bias vector\n",
       "        (see [initializers](../initializers.md)).\n",
       "    unit_forget_bias: Boolean.\n",
       "        If True, add 1 to the bias of the forget gate at initialization.\n",
       "        Use in combination with `bias_initializer=\"zeros\"`.\n",
       "        This is recommended in [Jozefowicz et al. (2015)](\n",
       "        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n",
       "    kernel_regularizer: Regularizer function applied to\n",
       "        the `kernel` weights matrix\n",
       "        (see [regularizer](../regularizers.md)).\n",
       "    recurrent_regularizer: Regularizer function applied to\n",
       "        the `recurrent_kernel` weights matrix\n",
       "        (see [regularizer](../regularizers.md)).\n",
       "    bias_regularizer: Regularizer function applied to the bias vector\n",
       "        (see [regularizer](../regularizers.md)).\n",
       "    activity_regularizer: Regularizer function applied to\n",
       "        the output of the layer (its \"activation\").\n",
       "        (see [regularizer](../regularizers.md)).\n",
       "    kernel_constraint: Constraint function applied to\n",
       "        the `kernel` weights matrix\n",
       "        (see [constraints](../constraints.md)).\n",
       "    recurrent_constraint: Constraint function applied to\n",
       "        the `recurrent_kernel` weights matrix\n",
       "        (see [constraints](../constraints.md)).\n",
       "    bias_constraint: Constraint function applied to the bias vector\n",
       "        (see [constraints](../constraints.md)).\n",
       "    return_sequences: Boolean. Whether to return the last output\n",
       "        in the output sequence, or the full sequence.\n",
       "    go_backwards: Boolean (default False).\n",
       "        If True, process the input sequence backwards.\n",
       "    stateful: Boolean (default False). If True, the last state\n",
       "        for each sample at index i in a batch will be used as initial\n",
       "        state for the sample of index i in the following batch.\n",
       "    dropout: Float between 0 and 1.\n",
       "        Fraction of the units to drop for\n",
       "        the linear transformation of the inputs.\n",
       "    recurrent_dropout: Float between 0 and 1.\n",
       "        Fraction of the units to drop for\n",
       "        the linear transformation of the recurrent state.\n",
       "\n",
       "# Input shape\n",
       "    - if data_format='channels_first'\n",
       "        5D tensor with shape:\n",
       "        `(samples, time, channels, rows, cols)`\n",
       "    - if data_format='channels_last'\n",
       "        5D tensor with shape:\n",
       "        `(samples, time, rows, cols, channels)`\n",
       "\n",
       "# Output shape\n",
       "    - if `return_sequences`\n",
       "         - if data_format='channels_first'\n",
       "            5D tensor with shape:\n",
       "            `(samples, time, filters, output_row, output_col)`\n",
       "         - if data_format='channels_last'\n",
       "            5D tensor with shape:\n",
       "            `(samples, time, output_row, output_col, filters)`\n",
       "    - else\n",
       "        - if data_format='channels_first'\n",
       "            4D tensor with shape:\n",
       "            `(samples, filters, output_row, output_col)`\n",
       "        - if data_format='channels_last'\n",
       "            4D tensor with shape:\n",
       "            `(samples, output_row, output_col, filters)`\n",
       "        where o_row and o_col depend on the shape of the filter and\n",
       "        the padding\n",
       "\n",
       "# Raises\n",
       "    ValueError: in case of invalid constructor arguments.\n",
       "\n",
       "# References\n",
       "    - [Convolutional LSTM Network: A Machine Learning Approach for\n",
       "      Precipitation Nowcasting](http://arxiv.org/abs/1506.04214v1)\n",
       "      The current implementation does not include the feedback loop on the\n",
       "      cells output\n",
       "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/convolutional_recurrent.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConvLSTM2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
