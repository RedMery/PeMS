{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras import losses\n",
    "import shutil\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = os.listdir('dataImage/')\n",
    "fileNames.sort()\n",
    "filePaths = [os.path.join(os.path.abspath('dataImage/'), fileName) for fileName in fileNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 发送消息\n",
    "from twilio.rest import Client\n",
    "def sendMessage(sentInfo):\n",
    "    # Your Account SID from twilio.com/console\n",
    "    account_sid = \"ACc807eaedbd160252f16250798533706f\"\n",
    "    # Your Auth Token from twilio.com/console\n",
    "    auth_token  = \"3199024dc7475dcd653f98a40cd16648\"\n",
    "    client = Client(account_sid, auth_token)\n",
    "    message = client.messages.create(\n",
    "    # 这里中国的号码前面需要加86\n",
    "        to=\"8613246857840\", \n",
    "        from_=\"(425) 341-1571 \",\n",
    "        body=\"{0}\".format(sentInfo))\n",
    "#     print(message.sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生数据，返回数据为(timeStep, 56, 56, 1),内容索引为\n",
    "def dataGenTimeStep(filePaths, start, timeStep):\n",
    "    l = []\n",
    "    for i in range(timeStep):\n",
    "        data = np.load(filePaths[start+i])\n",
    "#         print (filePaths[start+i], '读取完毕')  # 输出一个batch中的文件\n",
    "        data = data[:,:,1]\n",
    "        data = data[:,:,np.newaxis]\n",
    "        l.append(data)\n",
    "    dataArray = np.array(l)\n",
    "    return dataArray\n",
    "\n",
    "# 产生一个batch_size的数据\n",
    "def dataGenBatchSize(filePaths, timeStep, start, predictFutureTime, batch_size):\n",
    "    l = []\n",
    "    m = []\n",
    "    for i in range(batch_size):    \n",
    "        data = dataGenTimeStep(filePaths, start + i , timeStep)\n",
    "        l.append(data)\n",
    "        targetIndex = start + i + timeStep - 1 + predictFutureTime/5\n",
    "        targetIndex = int(targetIndex)\n",
    "        target = np.load(filePaths[targetIndex])\n",
    "#         print ('索引文件为', filePaths[targetIndex])    #测试目标文件是否正确\n",
    "        target = target[:,:,1]\n",
    "        target = target[:,:,np.newaxis]\n",
    "        m.append(target)\n",
    "    data = np.array(l)\n",
    "    target = np.array(m)\n",
    "    return (data, target)\n",
    "\n",
    "# timeStep---预测的时间步\n",
    "# predictFutureTime--预测未来的时间\n",
    "# batch_size -- 每次输入的batch，更新一次\n",
    "def dataGenerate(filePaths, timeStep, predictFutureTime, batch_size):\n",
    "    while True:\n",
    "        end_index = len(filePaths) - timeStep - 3\n",
    "        L = np.arange(0, end_index, batch_size)\n",
    "        for start in L:\n",
    "            if start < L[-1]:\n",
    "                data, target = dataGenBatchSize(filePaths, timeStep, start, predictFutureTime, batch_size)\n",
    "            if start == L[-1]:\n",
    "                batch_size_ = end_index - L[-1] + 1\n",
    "                data, target = dataGenBatchSize(filePaths, timeStep, start, predictFutureTime, batch_size_)\n",
    "            yield (data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数 MSPE\n",
    "\n",
    "def mean_squared_percentage_error(y_true, y_pred):\n",
    "    if not K.is_tensor(y_pred):\n",
    "        y_pred = K.constant(y_pred, dtype = 'float64')\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "    return K.mean(K.square((y_pred - y_true)/K.clip(y_true, K.epsilon(), None)))\n",
    "\n",
    "\n",
    "# 定义评价函数metrics, MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    if not K.is_tensor(y_pred):\n",
    "        y_pred = K.constant(y_pred, dtype = 'float64')\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "    return K.mean(K.abs((y_true - y_pred) / K.clip(K.abs(y_true), K.epsilon(), None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [mean_absolute_percentage_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Sequential()\n",
    "\n",
    "seq.add(ConvLSTM2D(filters = 32, kernel_size = (3, 3),\n",
    "                   input_shape = (None, 56, 56, 1),\n",
    "                   padding = 'same', return_sequences = True))\n",
    "\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "\n",
    "seq.add(ConvLSTM2D(filters = 32, kernel_size = (3, 3),\n",
    "                   padding = 'same', return_sequences = True))\n",
    "\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters = 1, kernel_size = (3, 3),\n",
    "                   padding = 'same', return_sequences = False))\n",
    "\n",
    "## 损失函数定义为 mean_squared_percentage_error\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "seq.compile(loss = mean_squared_percentage_error, optimizer = adam, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath = \"model\\{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "# 中途训练效果提升, 则将文件保存, 每提升一次, 保存一次\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_7 (ConvLSTM2D)  (None, None, 56, 56, 32)  38144     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, 56, 56, 32)  128       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_8 (ConvLSTM2D)  (None, None, 56, 56, 32)  73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, None, 56, 56, 32)  128       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_9 (ConvLSTM2D)  (None, 56, 56, 1)         1192      \n",
      "=================================================================\n",
      "Total params: 113,448\n",
      "Trainable params: 113,320\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq.summary()\n",
    "# plot_model(seq, show_layer_names = True, show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq.metrics_names\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.mape = []\n",
    "\n",
    "#         print ()\n",
    "#         print ('*'*10, 'batch begin', '*'*10)\n",
    "#         print (\"logs:\", logs)\n",
    "#         print ('*'*10, 'batch begin', '*'*10)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.mape.append(logs.get('mean_absolute_percentage_error'))\n",
    "\n",
    "#         print ()\n",
    "#         print ('*'*10, 'batch end', '*'*10)\n",
    "#         print (\"logs:\", logs)\n",
    "#         print ('*'*10, 'batch end', '*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(filePaths)\n",
    "EPOCHS = 6\n",
    "timeStep = 3\n",
    "predictFutureTime = 15\n",
    "batch_size = 16\n",
    "\n",
    "history_train = LossHistory()\n",
    "\n",
    "# 训练数据集\n",
    "# dataGenerate(filePaths, timeStep, predictFutureTime, batch_size)\n",
    "trainStart = 0\n",
    "trainEnd = math.floor(total/10*8)\n",
    "train_steps = math.ceil((trainEnd - trainStart)/batch_size)\n",
    "train_file_paths = filePaths[trainStart:trainEnd]\n",
    "train_generator = dataGenerate(train_file_paths, timeStep, predictFutureTime, batch_size)\n",
    "\n",
    "# 验证数据\n",
    "valStart = math.floor(total/10*8)\n",
    "valEnd = math.floor(total/10*9)\n",
    "val_steps = math.ceil((valEnd - valStart)/batch_size)\n",
    "val_file_paths = filePaths[valStart:valEnd]\n",
    "val_generator = dataGenerate(val_file_paths, timeStep, predictFutureTime, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'mean_absolute_percentage_error']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/redmery/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/redmery/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/6\n",
      "1296/1296 [==============================] - 75s 58ms/step - loss: 586011103.7004 - mean_absolute_percentage_error: 279.0171 - val_loss: 139.9652 - val_mean_absolute_percentage_error: 4.4162\n",
      "\n",
      "Epoch 00001: saving model to weights-improvement-01-139.97.hdf5\n",
      "Epoch 2/6\n",
      "1296/1296 [==============================] - 73s 57ms/step - loss: 351478.9355 - mean_absolute_percentage_error: 3.5284 - val_loss: 5.8417 - val_mean_absolute_percentage_error: 2.2486\n",
      "\n",
      "Epoch 00002: saving model to weights-improvement-02-5.84.hdf5\n",
      "Epoch 3/6\n",
      " 336/1296 [======>.......................] - ETA: 52s - loss: 107347.4662 - mean_absolute_percentage_error: 2.3777"
     ]
    }
   ],
   "source": [
    "historyTrainEpoch = seq.fit_generator(generator = train_generator, steps_per_epoch = train_steps, \\\n",
    "                      epochs = EPOCHS, callbacks = [history_train, checkpoint], validation_data = val_generator, \\\n",
    "                      validation_steps = val_steps, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'historyTrainEpoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0dbd5fc91167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## save history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistoryTrainEpoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## transfom the data to json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMyEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'historyTrainEpoch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "## save history\n",
    "print (historyTrainEpoch.history)\n",
    "\n",
    "## transfom the data to json\n",
    "class MyEncoder(json.JSONEncoder):\n",
    " \n",
    "    def default(self, obj):\n",
    "        \"\"\"\n",
    "        只要检查到了是bytes类型的数据就把它转为str类型\n",
    "        :param obj:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(obj, np.float32):\n",
    "            return str(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# 保存训练过程中的loss，以及验证过程中的loss、mspe\n",
    "with open('process_data_each_time.json', 'w') as f:\n",
    "    json.dump(historyTrainEpoch.history, f, cls=MyEncoder)\n",
    "np.save('history_train_loss.npy', np.array(history_train.losses))\n",
    "np.save('history_train_mape.npy', np.array(history_train.mape))\n",
    "\n",
    "# 保存模型(使用callback对每轮的迭代进行保存，因此不用该方法保存)\n",
    "# if os.path.exists('model'):\n",
    "#     shutil.rmtree('model')\n",
    "# os.makedirs('model')\n",
    "# model_path = os.path.join('model','model.h5')\n",
    "# seq.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  加载模型(模型中自定义的函数，需要使用custom_objects进行指明)\n",
    "# model_path = os.path.join('model/model' ,'model.h5')\n",
    "# seq = load_model(model_path, custom_objects={'my_loss': my_loss,'mean_squared_percentage_error':mean_squared_percentage_error})\n",
    "# history_evaluate = LossHistory()\n",
    "\n",
    "# # 测试数据\n",
    "# testStart = math.floor(total/10*9)\n",
    "# testEnd = math.floor(total)\n",
    "# test_steps = math.ceil((testEnd - testStart)/batch_size)\n",
    "# test_file_paths = filePaths[testStart:testEnd]\n",
    "# test_data = dataGenerate(test_file_paths, timeStep, predictFutureTime, batch_size)\n",
    "\n",
    "# seq.summary()\n",
    "# result_evaluate = seq.evaluate_generator(test_data, steps = test_steps, \\\n",
    "#                                               callbacks = [history_evaluate], verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
